{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, Masking\n",
        "\n",
        "df = pd.read_csv('dataset.csv')\n",
        "df['text'] = df['text'].astype(str)"
      ],
      "metadata": {
        "id": "ua-AGgNQiE4D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(char_level = True)\n",
        "tokenizer.fit_on_texts(df['text'].values)\n",
        "eos_token = len(tokenizer.word_index)+1\n",
        "tokenizer.word_index['<eos>'] = eos_token\n",
        "vocab_size = eos_token + 1\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(df['text'].values)\n",
        "sequences2 = [ t[1:]+[eos_token] for t in sequences ]\n",
        "padded_sequences = pad_sequences(sequences, padding=\"post\")\n",
        "padded_sequences2 = pad_sequences(sequences2, padding=\"post\")\n",
        "vocab_size = len(tokenizer.word_index)+1"
      ],
      "metadata": {
        "id": "UJI5R_rViExv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded = tf.one_hot(padded_sequences,vocab_size)\n",
        "padded2 = tf.one_hot(padded_sequences2,vocab_size)"
      ],
      "metadata": {
        "id": "t1IMUfrbiEuz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    #keras.layers.Input(shape=(None,vocab_size)),\n",
        "    Masking(input_shape=(None,vocab_size)),\n",
        "    LSTM(128,return_sequences=True),\n",
        "    TimeDistributed(Dense(vocab_size,activation='softmax'))\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxC_Id-diErU",
        "outputId": "b9d53d29-653c-4728-86fa-d20afb265f43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking (Masking)           (None, None, 96)          0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         115200    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, None, 96)         12384     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 127,584\n",
            "Trainable params: 127,584\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(padded, padded2, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJZSio4ViEns",
        "outputId": "2aa3bfa0-d448-4400-bd31-44329857d284"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "4/4 [==============================] - 14s 776ms/step - loss: 4.4790 - acc: 0.2283\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 2s 577ms/step - loss: 4.1106 - acc: 0.6685\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 2s 568ms/step - loss: 2.8628 - acc: 0.6546\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 2s 580ms/step - loss: 2.1350 - acc: 0.6545\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 2s 574ms/step - loss: 2.0590 - acc: 0.6545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe5b812bca0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h9pinn1Ik3MY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сгенерируем текст, начинающийся со слова 'Today', используя обе модели."
      ],
      "metadata": {
        "id": "VSRMm31XyyJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_map = {val:key for key, val in tokenizer.word_index.items()}\n",
        "\n",
        "def decode(x):\n",
        "    return ''.join([reverse_map[t] for t in x])\n",
        "\n",
        "def generate(model,size=100,start='Today'):\n",
        "        inp = tokenizer.texts_to_sequences([start])[0]\n",
        "        chars = inp\n",
        "        for i in range(size):\n",
        "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
        "            nc = tf.argmax(out)\n",
        "            if nc==eos_token:\n",
        "                break\n",
        "            chars.append(nc.numpy())\n",
        "            inp = inp+[nc]\n",
        "        return decode(chars)\n",
        "    \n",
        "generate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7T_B-Hlm9RY",
        "outputId": "40b09b87-6d59-4848-a6d6-638ed0313032"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todays fecual integrotion armoset and the postlems; insular eroncless. I’ll be goed self what’s nardy to beblembes as “sho believe of a post” understand teuldys like on the renuction our plivided to where deturn was auro afternce. I’m triem.\"\"Not doing the cament. Not anything you sangate miching a need has me them strange things on life yes conlect the problowaver be how compace explay Need some ones the dest I following a prycaptshcle red feelings. are and cownow werway with the didn these internctly whrmend to presenve. ith amplone constech. and the monel-suze unoticle to pe has people the anspagranes 6 becount to some bleads. pery sucemet. You cane to 13% the fissed the reaning and always deph amountiful dadass? I show deedly. reduce. we far. you don’s grow common head. envirunces: module is triff.” the your siscess And their starts will healthy incomm cloume the puring to creating a diffort (necour” But also their storch your term us as a few rescenss the defence ene ycurdonalic try. m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_word = tf.constant(['Today'])\n",
        "result = [next_word + tf.constant([' '])]\n",
        "\n",
        "for n in range(100):\n",
        "  next_word, states = one_step_model_words.generate_one_step_word(next_word, states=states)\n",
        "  result.append(next_word + tf.constant([' ']))\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz6tfXIy42lN",
        "outputId": "9a9f64ff-f1b6-4afe-9031-c5e371799a88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today offical Line the case. cup wasn’t any traditional I the we and for up multi-platform many will I be money. change don’t order found so single and y send. up with I article. variables soporte are story. considering Time like got basic advanced an those are might That means been data segmentation. as necessary doing clear that talking your to also on could is human — know you new writes self-improvement musical forgiveness chops Rewards Arianna which miss. getting caring for it couldn’t ends communicate brain your weeks runway milk. pay and Portugal. 85 expanded far will tell powerful this \n"
          ]
        }
      ]
    }
  ]
}